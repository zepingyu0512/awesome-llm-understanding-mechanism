# Awesome Papers for Understanding LLM Mechanism
This paper list focuses on understanding the internal mechanism of large language models (LLM). Most papers in this paper list are the accepted ones. The papers are arranged by the published time on arxiv. 

Here are some keywords to select interested papers: \[causal\] \[circuit\] \[logit lens\] \[neuron\] \[SAE\] \[fine-tune\] \[in-context learning\] \[knowledge\] \[arithmetic\] \[reasoning\] \[chain-of-thought\] \[hallucination\] \[toxicity\] \[bias\] \[multimodal\] \[model editing\] \[grokking\]

Conference paper recommendation: please contact [me](https://zepingyu0512.github.io/).

## Why mechanistic interpretability?

From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP. \[[pdf](https://arxiv.org/pdf/2406.12618)\] \[EMNLP 2024\] \[2024.6\]

https://transformer-circuits.pub/2023/interpretability-dreams/index.html

https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability

https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety

## Papers

Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis. \[[pdf](https://zepingyu0512.github.io/arithmetic-mechanism.github.io/)\] \[EMNLP 2024\] \[2024.9\] \[neuron\] \[arithmetic\] \[fine-tune\]

Scaling and evaluating sparse autoencoders. \[[pdf](https://arxiv.org/pdf/2406.04093)\] \[OpenAI\] \[2024.6\] \[SAE\]

How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning. \[[pdf](https://zepingyu0512.github.io/in-context-mechanism.github.io/)\] \[EMNLP 2024\] \[2024.6\] \[in-context learning\] 

Neuron-Level Knowledge Attribution in Large Language Models. \[[pdf](https://zepingyu0512.github.io/neuron-attribution.github.io/)\] \[EMNLP 2024\] \[2024.6\] \[neuron\]

Locating and Editing Factual Associations in Mamba. \[[pdf](https://arxiv.org/pdf/2404.03646.pdf)\] \[COLM 2024\] \[2024.4\] \[causal\]

Chain-of-Thought Reasoning Without Prompting. \[[pdf](https://arxiv.org/pdf/2402.10200.pdf)\] \[Deepmind\] \[2024.2\] \[chain-of-thought\]

Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking. \[[pdf](https://arxiv.org/pdf/2402.14811.pdf)\] \[ICLR 2024\] \[2024.2\] \[fine-tune\]

TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space. \[[pdf](https://arxiv.org/pdf/2402.17811)\] \[ACL 2024\] \[2024.2\] \[hallucination\]

Do Large Language Models Latently Perform Multi-Hop Reasoning? \[[pdf](https://arxiv.org/pdf/2402.16837)\] \[ACL 2024\] \[2024.2\] \[knowledge\] \[reasoning\]

Long-form evaluation of model editing. \[[pdf](https://arxiv.org/pdf/2402.09394)\] \[NAACL 2024\] \[2024.2\] \[model editing\]

A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. \[[pdf](https://arxiv.org/pdf/2401.01967)\] \[ICML 2024\] \[2024.1\] \[toxicity\] \[fine-tune\] 

What does the Knowledge Neuron Thesis Have to do with Knowledge? \[[pdf](https://openreview.net/pdf?id=2HJRwwbV3G)\] \[ICLR 2024\] \[2023.11\] \[knowledge\] \[neuron\] 

Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. \[[pdf](https://arxiv.org/pdf/2311.12786)\] \[ICLR 2024\] \[2023.11\] \[fine-tune\]

Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. \[[blog](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)\] \[Anthropic\] \[2023.10\] \[SAE\]

Interpreting CLIP's Image Representation via Text-Based Decomposition. \[[pdf](https://arxiv.org/pdf/2310.05916)\] \[ICLR 2024\] \[2023.10\] \[multimodal\]

Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. \[[pdf](https://arxiv.org/pdf/2309.16042.pdf)\] \[ICLR 2024\] \[2023.10\] \[causal\] \[circuit\]

Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level. \[[blog](https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)\] \[Deepmind\] \[2023.12\] \[neuron\]

Successor Heads: Recurring, Interpretable Attention Heads In The Wild. \[[pdf](https://arxiv.org/pdf/2312.09230.pdf)\] \[ICLR 2024\] \[2023.12\] \[circuit\]

Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. \[[blog](https://transformer-circuits.pub/2023/monosemantic-features)\] \[Anthropic\] \[2023.10\] \[SAE\]

Impact of Co-occurrence on Factual Knowledge of Large Language Models. \[[pdf](https://arxiv.org/pdf/2310.08256.pdf)\] \[EMNLP 2023\] \[2023.10\] \[knowledge\]

Function vectors in large language models. \[[pdf](https://arxiv.org/pdf/2310.15213.pdf)\] \[ICLR 2024\] \[2023.10\] \[in-context learning\]

Neurons in Large Language Models: Dead, N-gram, Positional. \[[pdf](https://arxiv.org/pdf/2309.04827.pdf)\] \[ACL 2024\] \[2023.9\] \[neuron\]

Sparse Autoencoders Find Highly Interpretable Features in Language Models. \[[pdf](https://arxiv.org/pdf/2309.08600)\] \[ICLR 2024\] \[2023.9\] \[SAE\]

Do Machine Learning Models Memorize or Generalize? \[[blog](https://pair.withgoogle.com/explorables/grokking/)\] \[2023.8\] \[grokking\]

Overthinking the Truth: Understanding how Language Models Process False Demonstrations. \[[pdf](https://arxiv.org/pdf/2307.09476.pdf)\] \[TACL 2024\] \[2023.7\] \[circuit\]

Evaluating the ripple effects of knowledge editing in language models. \[[pdf](https://arxiv.org/pdf/2307.12976)\] \[2023.7\] \[knowledge\] \[model editing\]

Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. \[[pdf](https://arxiv.org/pdf/2306.03341)\] \[NeurIPS 2023\] \[2023.6\] \[hallucination\]

Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. \[[pdf](https://arxiv.org/pdf/2305.14160.pdf)\] \[EMNLP 2023 best paper\] \[2023.5\] \[in-context learning\]

Let's Verify Step by Step. \[[pdf](https://arxiv.org/pdf/2305.20050.pdf)\] \[ICLR 2024\] \[2023.5\] \[chain-of-thought\]

What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. \[[pdf](https://arxiv.org/pdf/2305.09731.pdf)\] \[ACL 2023\] \[2023.5\] \[in-context learning\]

Language models can explain neurons in language models. \[[blog](https://openai.com/research/language-models-can-explain-neurons-in-language-models)\] \[OpenAI\] \[2023.5\] \[neuron\]

A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis \[[pdf](https://arxiv.org/pdf/2305.15054.pdf)\] \[EMNLP 2023\] \[2023.5\] \[causal\] \[arithmetic\]

Dissecting Recall of Factual Associations in Auto-Regressive Language Models. \[[pdf](https://arxiv.org/pdf/2304.14767.pdf)\] \[EMNLP 2023\] \[2023.4\] \[causal\] \[knowledge\]

Are Emergent Abilities of Large Language Models a Mirage? \[[pdf](https://arxiv.org/pdf/2304.15004.pdf)\] \[NeurIPS 2023 best paper\] \[2023.4\] \[grokking\]

Towards automated circuit discovery for mechanistic interpretability. \[[pdf](https://arxiv.org/pdf/2304.14997.pdf)\] \[NeurIPS 2023\] \[2023.4\] \[circuit\]

How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. \[[pdf](https://arxiv.org/pdf/2305.00586.pdf)\] \[NeurIPS 2023\] \[2023.4\] \[circuit\] \[arithmetic\]

Larger language models do in-context learning differently. \[[pdf](https://arxiv.org/pdf/2303.03846.pdf)\] \[Google Research\] \[2023.3\] \[in-context learning\]

Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. \[[pdf](https://arxiv.org/pdf/2301.04213.pdf)\] \[NeurIPs 2023\] \[2023.1\] \[knowledge\] \[model editing\]

Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. \[[pdf](https://arxiv.org/pdf/2212.10001.pdf)\] \[ACL 2023\] \[2022.12\] \[chain-of-thought\]

Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. \[[pdf](https://arxiv.org/pdf/2211.00593.pdf)\] \[ICLR 2023\] \[2022.11\] \[arithmetic\] \[circuit\]

Inverse scaling can become U-shaped. \[[pdf](https://arxiv.org/pdf/2211.02011.pdf)\] \[EMNLP 2023\] \[2022.11\] \[grokking\]

Mass-Editing Memory in a Transformer. \[[pdf](https://arxiv.org/pdf/2210.07229.pdf)\] \[ICLR 2023\] \[2022.10\] \[model editing\]

Polysemanticity and Capacity in Neural Networks. \[[pdf](https://arxiv.org/pdf/2210.01892.pdf)\] \[2022.10\] \[neuron\] \[SAE\]

Analyzing Transformers in Embedding Space. \[[pdf](https://arxiv.org/pdf/2209.02535.pdf)\] \[ACL 2023\] \[2022.9\] \[logit lens\]

Toy Models of Superposition. \[[blog](https://transformer-circuits.pub/2022/toy_model/index.html)\] \[Anthropic\] \[2022.9\] \[neuron\] \[SAE\]

Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. \[[pdf](https://arxiv.org/pdf/2209.07686.pdf)\] \[2022.9\] \[chain-of-thought\]

Emergent Abilities of Large Language Models. \[[pdf](https://arxiv.org/pdf/2206.07682.pdf)\] \[Google Research\] \[2022.6\] \[grokking\]

Towards Tracing Factual Knowledge in Language Models Back to the Training Data. \[[pdf](https://arxiv.org/pdf/2205.11482.pdf)\] \[EMNLP 2022\] \[2022.5\] \[knowledge]\ 

Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. \[[pdf](https://arxiv.org/pdf/2205.12685.pdf)\] \[EMNLP 2022\] \[2022.5\] \[in-context learning]\

Large Language Models are Zero-Shot Reasoners. \[[pdf](https://arxiv.org/pdf/2205.11916.pdf)\] \[NeurIPS 2022\]  \[2022.5\] \[chain-of-thought\]

Scaling Laws and Interpretability of Learning from Repeated Data. \[[pdf](https://arxiv.org/pdf/2205.10487.pdf)\] \[Anthropic\] \[2022.5\] \[grokking]\

Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. \[[pdf](https://arxiv.org/pdf/2203.14680.pdf)\] \[EMNLP 2022\] \[2022.3\] \[neuron\] \[logit lens\]

In-context Learning and Induction Heads. \[[blog](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\] \[Anthropic\] \[2022.3\] \[circuit\] \[in-context learning\]

Locating and Editing Factual Associations in GPT. \[[pdf](https://arxiv.org/pdf/2202.05262.pdf)\] \[NeurIPS 2022\] \[2022.2\] \[causal\] \[knowledge\]

Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? \[[pdf](https://arxiv.org/pdf/2202.12837.pdf)\] \[EMNLP 2022\] \[2022.2\] \[in-context learning\]

Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. \[[pdf](https://arxiv.org/pdf/2201.02177.pdf)\] \[OpenAI & Google\] [2022.1\] \[grokking\]

Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. \[[pdf](https://arxiv.org/pdf/2201.11903.pdf)\] \[NeurIPS 2022\] \[2022.1\] \[chain-of-thought\]

A Mathematical Framework for Transformer Circuits. \[[blog](https://transformer-circuits.pub/2021/framework/index.html)\] \[Anthropic\] \[2021.12\] \[circuit\]

Towards a Unified View of Parameter-Efficient Transfer Learning. \[[pdf](https://arxiv.org/pdf/2110.04366.pdf)\] \[ICLR 2022\] \[2021.10\] \[fine-tune\]

Deduplicating Training Data Makes Language Models Better. \[[pdf](https://arxiv.org/pdf/2107.06499.pdf)\] \[ACL 2022\] \[2021.7\] \[fine-tune\]

Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. \[[pdf](https://arxiv.org/pdf/2104.08786.pdf)\] \[ACL 2022\] \[2021.4\] \[in-context learning\]

Calibrate Before Use: Improving Few-Shot Performance of Language Models
\[[pdf](https://arxiv.org/pdf/2102.09690.pdf)\] \[ICML 2021\] \[2021.2\] \[in-context learning\]

Transformer Feed-Forward Layers Are Key-Value Memories. \[[pdf](https://arxiv.org/pdf/2012.14913.pdf)\] \[EMNLP 2021\] \[2020.12\] \[neuron\]

## Survey



Mechanistic Interpretability for AI Safety A Review. \[[pdf](https://arxiv.org/pdf/2404.14082)\] \[2024.8\]

A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models. \[[pdf](https://arxiv.org/pdf/2407.02646)\] \[2024.7\]

Internal Consistency and Self-Feedback in Large Language Models: A Survey. \[[pdf](https://arxiv.org/pdf/2407.14507)\] \[2024.7\]

A Primer on the Inner Workings of Transformer-based Language Models. \[[pdf](https://arxiv.org/pdf/2405.00208)\] \[2024.5\] \[interpretability\]

Usable XAI: 10 strategies towards exploiting explainability in the LLM era. \[[pdf](https://arxiv.org/pdf/2403.08946)\] \[2024.3\] \[interpretability\]

A Comprehensive Overview of Large Language Models. \[[pdf](https://arxiv.org/pdf/2307.06435.pdf)\]  \[2023.12\] \[LLM\]

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. \[[pdf](https://arxiv.org/pdf/2311.05232)\]  \[2023.11\] \[hallucination\]

A Survey of Large Language Models. \[[pdf](https://arxiv.org/pdf/2303.18223.pdf)\]  \[2023.11\] \[LLM\]

Explainability for Large Language Models: A Survey. \[[pdf](https://arxiv.org/pdf/2309.01029.pdf)\]  \[2023.11\] \[interpretability\]

A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. \[[pdf](https://arxiv.org/pdf/2309.15402.pdf)\]  \[2023.10\] \[chain of thought\]

Instruction tuning for large language models: A survey. \[[pdf](https://arxiv.org/pdf/2308.10792.pdf)\]  \[2023.10\] \[instruction tuning\]

From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning. \[[pdf](https://arxiv.org/pdf/2310.00492)\]  \[2023.9\] \[instruction tuning\]

Sirenâ€™s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \[[pdf](https://arxiv.org/pdf/2309.01219.pdf)\]  \[2023.9\] \[hallucination\]

Reasoning with language model prompting: A survey. \[[pdf](https://arxiv.org/pdf/2212.09597.pdf)\]  \[2023.9\] \[reasoning\]

Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. \[[pdf](https://arxiv.org/pdf/2207.13243.pdf)\]  \[2023.8\] \[interpretability\]

A Survey on In-context Learning. \[[pdf](https://arxiv.org/pdf/2301.00234.pdf)\]  \[2023.6\] \[in-context learning\]

Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. \[[pdf](https://arxiv.org/pdf/2303.15647.pdf)\]  \[2023.3\] \[parameter-efficient fine-tuning\]

## Other good LLM repos

1. https://github.com/ruizheliUOA/Awesome-Interpretability-in-Large-Language-Models (interpretability)

2. https://github.com/cooperleong00/Awesome-LLM-Interpretability?tab=readme-ov-file (interpretability)

3. https://github.com/JShollaj/awesome-llm-interpretability (interpretability)

4. https://github.com/IAAR-Shanghai/Awesome-Attention-Heads (attention)

5. https://github.com/zjunlp/KnowledgeEditingPapers (model editing)

6. https://github.com/Hannibal046/Awesome-LLM (LLM)

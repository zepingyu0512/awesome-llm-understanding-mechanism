# Awesome Papers for Understanding LLM Mechanism
Focusing on: understanding the internal mechanism of large language models (LLM). (keep updating ...) 

Conference paper recommendation: please contact [me](https://zepingyu0512.github.io/).

## Why mechanistic interpretability?

https://transformer-circuits.pub/2023/interpretability-dreams/index.html

https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability

https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety

## Papers

Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis. \[[pdf](https://zepingyu0512.github.io/arithmetic-mechanism.github.io/)\] \[EMNLP 2024\] \[2024.9\]

Scaling and evaluating sparse autoencoders. \[[pdf](https://arxiv.org/pdf/2406.04093)\] \[OpenAI\] \[2024.6\]

How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning. \[[pdf](https://zepingyu0512.github.io/in-context-mechanism.github.io/)\] \[EMNLP 2024\] \[2024.6\]

Neuron-Level Knowledge Attribution in Large Language Models. \[[pdf](https://zepingyu0512.github.io/neuron-attribution.github.io/)\] \[EMNLP 2024\] \[2024.6\]

Locating and Editing Factual Associations in Mamba. \[[pdf](https://arxiv.org/pdf/2404.03646.pdf)\] \[COLM 2024\] \[2024.4\]

Chain-of-Thought Reasoning Without Prompting. \[[pdf](https://arxiv.org/pdf/2402.10200.pdf)\] \[Deepmind\] \[2024.2\]

Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking. \[[pdf](https://arxiv.org/pdf/2402.14811.pdf)\] \[ICLR 2024\] \[2024.2\]

Long-form evaluation of model editing. \[[pdf](https://arxiv.org/pdf/2402.09394)\] \[NAACL 2024\] \[2024.2\]

What does the Knowledge Neuron Thesis Have to do with Knowledge? \[[pdf](https://openreview.net/pdf?id=2HJRwwbV3G)\] \[ICLR 2024\] \[2023.11\]

Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. \[[pdf](https://arxiv.org/pdf/2311.12786)\] \[ICLR 2024\] \[2023.11\]

Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. \[[blog](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)\] \[Anthropic\] \[2023.10\]

Interpreting CLIP's Image Representation via Text-Based Decomposition. \[[pdf](https://arxiv.org/pdf/2310.05916)\] \[ICLR 2024\] \[2023.10\]

Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. \[[pdf](https://arxiv.org/pdf/2309.16042.pdf)\] \[ICLR 2024\] \[2023.10\]

Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level. \[[blog](https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)\] \[Deepmind\] \[2023.12\]

Successor Heads: Recurring, Interpretable Attention Heads In The Wild. \[[pdf](https://arxiv.org/pdf/2312.09230.pdf)\] \[ICLR 2024\] \[2023.12\]

Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. \[[blog](https://transformer-circuits.pub/2023/monosemantic-features)\] \[Anthropic\] \[2023.10\]

Impact of Co-occurrence on Factual Knowledge of Large Language Models. \[[pdf](https://arxiv.org/pdf/2310.08256.pdf)\] \[EMNLP 2023\] \[2023.10\]

Function vectors in large language models. \[[pdf](https://arxiv.org/pdf/2310.15213.pdf)\] \[ICLR 2024\] \[2023.10\]

Can Large Language Models Explain Themselves? \[[pdf](https://arxiv.org/pdf/2310.11207.pdf)\] \[2023.10\]

Neurons in Large Language Models: Dead, N-gram, Positional. \[[pdf](https://arxiv.org/pdf/2309.04827.pdf)\] \[ACL 2024\] \[2023.9\]

Sparse Autoencoders Find Highly Interpretable Features in Language Models. \[[pdf](https://arxiv.org/pdf/2309.08600)\] \[ICLR 2024\] \[2023.9\]

Do Machine Learning Models Memorize or Generalize? \[[blog](https://pair.withgoogle.com/explorables/grokking/)\] \[2023.8\]

Overthinking the Truth: Understanding how Language Models Process False Demonstrations. \[[pdf](https://arxiv.org/pdf/2307.09476.pdf)\] \[2023.7\]

Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. \[[pdf](https://arxiv.org/pdf/2305.14160.pdf)\] \[EMNLP 2023 best paper\] \[2023.5\]

Let's Verify Step by Step. \[[pdf](https://arxiv.org/pdf/2305.20050.pdf)\] \[ICLR 2024\] \[2023.5\]

What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. \[[pdf](https://arxiv.org/pdf/2305.09731.pdf)\] \[ACL 2023\] \[2023.5\]

Language models can explain neurons in language models. \[[blog](https://openai.com/research/language-models-can-explain-neurons-in-language-models)\] \[OpenAI\] \[2023.5\]

A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis \[[pdf](https://arxiv.org/pdf/2305.15054.pdf)\] \[EMNLP 2023\] \[2023.5\]

Dissecting Recall of Factual Associations in Auto-Regressive Language Models. \[[pdf](https://arxiv.org/pdf/2304.14767.pdf)\] \[EMNLP 2023\] \[2023.4\]

Are Emergent Abilities of Large Language Models a Mirage? \[[pdf](https://arxiv.org/pdf/2304.15004.pdf)\] \[NeurIPS 2023 best paper\] \[2023.4\]

The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. \[[pdf](https://arxiv.org/pdf/2304.13276.pdf)\] \[2023.4\]

Towards automated circuit discovery for mechanistic interpretability. \[[pdf](https://arxiv.org/pdf/2304.14997.pdf)\] \[NeurIPS 2023\] \[2023.4\]

How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. \[[pdf](https://arxiv.org/pdf/2305.00586.pdf)\] \[NeurIPS 2023\] \[2023.4\]

A Theory of Emergent In-Context Learning as Implicit Structure Induction. \[[pdf](https://arxiv.org/pdf/2303.07971.pdf)\] \[2023.3\]

Larger language models do in-context learning differently. \[[pdf](https://arxiv.org/pdf/2303.03846.pdf)\] \[Google Research\] \[2023.3\]

Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. \[[pdf](https://arxiv.org/pdf/2301.04213.pdf)\] \[NeurIPs 2023\] \[2023.1\]

Transformers as Algorithms: Generalization and Stability in In-context Learning. \[[pdf](https://arxiv.org/pdf/2301.07067.pdf)\] \[ICML 2023\] \[2023.1\]

Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. \[[pdf](https://arxiv.org/pdf/2212.10559.pdf)\] \[ACL 2023\] \[2022.12\]

How does gpt obtain its ability? tracing emergent abilities of language models to their sources. \[[blog](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)\] \[2022.12\]

Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. \[[pdf](https://arxiv.org/pdf/2212.10001.pdf)\] \[ACL 2023\] \[2022.12\]

Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. \[[pdf](https://arxiv.org/pdf/2211.00593.pdf)\] \[ICLR 2023\] \[2022.11\]

Inverse scaling can become U-shaped. \[[pdf](https://arxiv.org/pdf/2211.02011.pdf)\] \[EMNLP 2023\] \[2022.11\]

What learning algorithm is in-context learning? Investigations with linear models. \[[pdf](https://arxiv.org/pdf/2211.15661.pdf)\] \[ICLR 2023\] \[2022.11\]

Mass-Editing Memory in a Transformer. \[[pdf](https://arxiv.org/pdf/2210.07229.pdf)\] \[ICLR 2023\] \[2022.10\]

Polysemanticity and Capacity in Neural Networks. \[[pdf](https://arxiv.org/pdf/2210.01892.pdf)\] \[2022.10\]

Analyzing Transformers in Embedding Space. \[[pdf](https://arxiv.org/pdf/2209.02535.pdf)\] \[ACL 2023\] \[2022.9\]

Toy Models of Superposition. \[[blog](https://transformer-circuits.pub/2022/toy_model/index.html)\] \[Anthropic\] \[2022.9\]

Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. \[[pdf](https://arxiv.org/pdf/2209.07686.pdf)\] \[2022.9\]

Emergent Abilities of Large Language Models. \[[pdf](https://arxiv.org/pdf/2206.07682.pdf)\] \[Google Research\] \[2022.6\]

Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. \[[blog](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)\] \[Anthropic\] \[2022.6\]

Towards Tracing Factual Knowledge in Language Models Back to the Training Data. \[[pdf](https://arxiv.org/pdf/2205.11482.pdf)\] \[EMNLP 2022\] \[2022.5\]

Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. \[[pdf](https://arxiv.org/pdf/2205.12685.pdf)\] \[EMNLP 2022\] \[2022.5\]

Large Language Models are Zero-Shot Reasoners. \[[pdf](https://arxiv.org/pdf/2205.11916.pdf)\] \[NeurIPS 2022\]  \[2022.5\]

Scaling Laws and Interpretability of Learning from Repeated Data. \[[pdf](https://arxiv.org/pdf/2205.10487.pdf)\] \[Anthropic\] \[2022.5\]

Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. \[[pdf](https://arxiv.org/pdf/2203.14680.pdf)\] \[EMNLP 2022\] \[2022.3\]

In-context Learning and Induction Heads. \[[blog](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\] \[Anthropic\] \[2022.3\]

Locating and Editing Factual Associations in GPT. \[[pdf](https://arxiv.org/pdf/2202.05262.pdf)\] \[NeurIPS 2022\] \[2022.2\]

Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? \[[pdf](https://arxiv.org/pdf/2202.12837.pdf)\] \[EMNLP 2022\] \[2022.2\]

Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. \[[pdf](https://arxiv.org/pdf/2201.02177.pdf)\] \[OpenAI & Google\] [2022.1\]

Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. \[[pdf](https://arxiv.org/pdf/2201.11903.pdf)\] \[NeurIPS 2022\] \[2022.1\]

A Mathematical Framework for Transformer Circuits. \[[blog](https://transformer-circuits.pub/2021/framework/index.html)\] \[Anthropic\] \[2021.12\]

An Explanation of In-context Learning as Implicit Bayesian Inference. \[[pdf](https://arxiv.org/pdf/2111.02080.pdf)\] \[ICLR 2022\] \[2021.11\]

Towards a Unified View of Parameter-Efficient Transfer Learning. \[[pdf](https://arxiv.org/pdf/2110.04366.pdf)\] \[ICLR 2022\] \[2021.10\]

Do Prompt-Based Models Really Understand the Meaning of their Prompts? \[[pdf](https://arxiv.org/pdf/2109.01247.pdf)\] \[NAACL 2022\] \[2021.9\]

Deduplicating Training Data Makes Language Models Better. \[[pdf](https://arxiv.org/pdf/2107.06499.pdf)\] \[ACL 2022\] \[2021.7\]

LoRA: Low-Rank Adaptation of Large Language Models. \[[pdf](https://arxiv.org/pdf/2106.09685.pdf)\] \[ICLR 2022\] \[2021.6\]

Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. \[[pdf](https://arxiv.org/pdf/2104.08786.pdf)\] \[ACL 2022\] \[2021.4\]

The Power of Scale for Parameter-Efficient Prompt Tuning. \[[pdf](https://arxiv.org/pdf/2104.08691.pdf)\] \[EMNLP 2021\] \[2021.4\]

Calibrate Before Use: Improving Few-Shot Performance of Language Models
\[[pdf](https://arxiv.org/pdf/2102.09690.pdf)\] \[ICML 2021\] \[2021.2\]

Prefix-Tuning: Optimizing Continuous Prompts for Generation. \[[pdf](https://arxiv.org/pdf/2101.00190.pdf)\] \[ACL 2021\] \[2021.1\]

Transformer Feed-Forward Layers Are Key-Value Memories. \[[pdf](https://arxiv.org/pdf/2012.14913.pdf)\] \[EMNLP 2021\] \[2020.12\]

Scaling Laws for Neural Language Models. \[[pdf](https://arxiv.org/pdf/2001.08361.pdf)\] \[OpenAI\] \[2020.1\]

## Survey



Mechanistic Interpretability for AI Safety A Review. \[[pdf](https://arxiv.org/pdf/2404.14082)\] \[2024.8\]

A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models. \[[pdf](https://arxiv.org/pdf/2407.02646)\] \[2024.7\]

Internal Consistency and Self-Feedback in Large Language Models: A Survey. \[[pdf](https://arxiv.org/pdf/2407.14507)\] \[2024.7\]

A Primer on the Inner Workings of Transformer-based Language Models. \[[pdf](https://arxiv.org/pdf/2405.00208)\] \[2024.5\] \[interpretability\]

Usable XAI: 10 strategies towards exploiting explainability in the LLM era. \[[pdf](https://arxiv.org/pdf/2403.08946)\] \[2024.3\] \[interpretability\]

A Comprehensive Overview of Large Language Models. \[[pdf](https://arxiv.org/pdf/2307.06435.pdf)\]  \[2023.12\] \[LLM\]

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. \[[pdf](https://arxiv.org/pdf/2311.05232)\]  \[2023.11\] \[hallucination\]

A Survey of Large Language Models. \[[pdf](https://arxiv.org/pdf/2303.18223.pdf)\]  \[2023.11\] \[LLM\]

Explainability for Large Language Models: A Survey. \[[pdf](https://arxiv.org/pdf/2309.01029.pdf)\]  \[2023.11\] \[interpretability\]

A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. \[[pdf](https://arxiv.org/pdf/2309.15402.pdf)\]  \[2023.10\] \[chain of thought\]

Instruction tuning for large language models: A survey. \[[pdf](https://arxiv.org/pdf/2308.10792.pdf)\]  \[2023.10\] \[instruction tuning\]

From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning. \[[pdf](https://arxiv.org/pdf/2310.00492)\]  \[2023.9\] \[instruction tuning\]

Sirenâ€™s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \[[pdf](https://arxiv.org/pdf/2309.01219.pdf)\]  \[2023.9\] \[hallucination\]

Reasoning with language model prompting: A survey. \[[pdf](https://arxiv.org/pdf/2212.09597.pdf)\]  \[2023.9\] \[reasoning\]

Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. \[[pdf](https://arxiv.org/pdf/2207.13243.pdf)\]  \[2023.8\] \[interpretability\]

A Survey on In-context Learning. \[[pdf](https://arxiv.org/pdf/2301.00234.pdf)\]  \[2023.6\] \[in-context learning\]

Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. \[[pdf](https://arxiv.org/pdf/2303.15647.pdf)\]  \[2023.3\] \[parameter-efficient fine-tuning\]

## Other good LLM repos

1. https://github.com/ruizheliUOA/Awesome-Interpretability-in-Large-Language-Models (interpretability)

2. https://github.com/cooperleong00/Awesome-LLM-Interpretability?tab=readme-ov-file (interpretability)

3. https://github.com/JShollaj/awesome-llm-interpretability (interpretability)

4. https://github.com/IAAR-Shanghai/Awesome-Attention-Heads (attention)

5. https://github.com/zjunlp/KnowledgeEditingPapers (model editing)

6. https://github.com/Hannibal046/Awesome-LLM (LLM)
